 tim peters wrote i ve run no experiments on training set size yet and won t hazard a guess as to how much is enough i m nearly certain that the h s i ve been using is way more than enough though okay i believe you each call to learn and to unlearn computes a new probability for every word in the database there s an official way to avoid that in the first two loops e g for msg in spam gb learn msg true false gb update probabilities i did that it s still really slow when you have thousands of messages in each of the last two loops the total of ham and total of spam in the learned set is invariant across loop trips and you could break into the abstraction to exploit that the only probabilities that actually change across those loop trips are those associated with the words in msg then the runtime for each trip would be proportional to the of words in the msg rather than the number of words in the database i hadn t tried that i figured it was better to find out if all but one testing had any appreciable value it looks like it doesn t so i ll forget about it another area for potentially fruitful study it s clear that the highest value indicators usually appear early in msgs and for spam there s an actual reason for that advertising has to strive to get your attention early so for example if we only bothered to tokenize the first of a msg would results get worse spammers could exploit this including a large mime part at the beginning of the message in pratice that would probably work fine sometimes an on topic message starts well but then rambles never i remember the time when i was ten years old and went down to the fishing hole with my buddies this guy named gordon had a really huge head wait maybe that was joe well no matter as i recall it was a hot day and everyone was tired human growth hormone girl with huge breasts blah blah blah 