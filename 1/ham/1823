 neil trained a classifier using sets with about ham and spam in each we re missing half his test run results due to a cmp py bug since fixed the before custom fiddling figures on the reported runs were false positive percentages total unique fp false negative percentages total unique fn the total unique figures counts all runs it s just the individual run fp and fn percentages we re missing for runs jeremy reported these before custom fiddling figures on sets with about ham and spam in each false positive percentages total unique fp false negative percentages total unique fn so things are clearly working much better for neil both reported significant improvements in both f n and f p rates by folding in more header lines neal added received analysis to the base tokenizer s header analysis and jeremy skipped the base tokenizer s header analysis completely but added base subject line like but case folded tokenization for almost all header lines excepting only received data x from and i suspect all those starting with x vm when i try random pairs of ham spam subsets in my test data i see false positive percentages total unique fp false negative percentages total unique fn this is much closer to what neil saw but still looks better another run on a disjoint random pairs looked much the same total unique fp rose to and fn fell to on a third run with another set of disjoint random pairs likewise with fp and fn so i m pretty confident that it s not going to matter which random subsets of i take from my data it s hard to conclude anything given jeremy s much worse results if they were in line with neil s results i d suspect that i ve over tuned the algorithm to statistical quirks in my corpora 