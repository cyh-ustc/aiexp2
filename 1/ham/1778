 so then tim peters is all like tim my tests train on about msgs and a binary pickle of the database is approaching million bytes that shrinks to under million bytes though if i delete all the wordinfo records with spamprob exactly equal to unknown spamprob such records aren t needed when scoring an unknown word gets a made up probability of unknown spamprob such records are only needed for training i ve noted before that a scoring only database can be leaner that s pretty good i wonder how much better you could do by using some custom pickler i just checked my little dbm file and found a lot of what i would call bloat import anydbm hammie d hammie persistentgrahambayes ham db db anydbm open ham db db neale len db neale ccopy reg n reconstructor nq x cclassifier nwordinfo nq x c builtin nobject nq x ntrq x ga xce xbc xfd x xbbok x k x k x g xe x x x x x x tb d wordinfo neale len d wordinfo neale wordinfo ignoring the fact that there are too many zeros in there the pickled version of that wordinfo object is over twice as large as the string representation so we could get a decrease in size just by using the string representation instead of the pickle right something about that logic seems wrong to me but i can t see what it is maybe pickling is good for heterogeneous data types but every value of our big dictionary is going to have the same type so there s a ton of redundancy i guess that explains why it compressed so well neale 