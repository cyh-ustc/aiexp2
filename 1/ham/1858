 how were these msgs broken up into the sets set in particular is giving the other sets severe problems and set blows the f n rate on everything it s predicting when the rates across runs within a training set vary by as much as a factor of it suggests there was systematic bias in the way the sets were chosen for example perhaps they were broken into sets by arrival time if that s what you did you should go back and break them into sets randomly instead if you did partition them randomly the wild variance across runs is mondo mysterious they weren t partitioned in any particular scheme i think i ll write a reshuffler and move them all around just in case fwiw i m using mh style folders with numbered files means you can just use mh tools to manipulate the sets for whatever reason there appear to be few of those in bruceg s spam collection i added code to strip uuencoded sections and pump out uuencode summary tokens instead i ll check it in it didn t make a significant difference on my usual test run a single spam in my set is now judged as ham by the other sets nothing else changed it does shrink the database size here by a few percent let us know whether it helps you i ll give it a go anthony baxter it s never too late to have a happy childhood 